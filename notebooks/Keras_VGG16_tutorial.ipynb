{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Flower Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import models, layers, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep networks have a large number of unknown parameters (in millions). The task of training a network is to find the optimum parameters using the training data. From linear algebra, we know that in order to solve an equation with three unknown parameters, we need three equations (data). Similarly, for finding all the unknown parameters accurately, we would need a lot of data (in millions). If we have very few data, we will get only approximate values for most of the parameters, which we donâ€™t want.\n",
    "\n",
    "However, it is difficult to get such huge labeled datasets for training the network, and, even if you get the data, it takes a large amount of time to train the network (hundreds of hours). Fortunately, we can leverage the models already trained on very large amounts of data for difficult tasks with thousands of classes. Most often we use these models as a starting point for our training process, instead of training our own model from scratch.\n",
    "\n",
    "This notebook uses the pre-trained model *VGG16* (Simonyan K, Zisserman A, 2015) with the initial weights as the ones obtained in the ImageNet ILSVRC-2014 competition. This code is based on the tutorial by Satya Mallick in www.learnopencv.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading pre-trained model\n",
    "\n",
    "Load the VGG Model along with the ImageNet weights. The *include_top=False* argument means it does not load the last two fully connected layers which act as the classifier, only the convolutional layers (feature extraction).\n",
    "The images we are going to load are all 256 x 256 pixels, with RGB (3 channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_conv = vgg16.VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(256,256,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading images\n",
    "\n",
    "We will use the *ImageDataGenerator* class from Keras to load the images and *flow_from_directory* function to generate batches of images and labels. The function returns a *DirectoryIterator* yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape (batch_size, target_size, channels) and y is a numpy array of corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/media/thabata/ExtraDrive1/INFNET/Flower_spotter/subset_train'\n",
    "validation_dir = '/media/thabata/ExtraDrive1/INFNET/Flower_spotter/subset_test'\n",
    " \n",
    "nTrain = 723\n",
    "nVal = 239\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 723 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#load images and generate batch for train data\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 239 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#load images and generate batch for validation data\n",
    " \n",
    "val_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Passing images through feature extraction layers\n",
    "\n",
    "Use *model.predict()* function to pass the images through the pre-trained network feature extraction layers. The 256x256x3 input becomes a 8x8x512 tensor in the final layer.\n",
    "Finally, we reshape the Tensor into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train data\n",
    "\n",
    "train_features = np.zeros(shape=(nTrain, 8, 8, 512))\n",
    "train_labels = np.zeros(shape=(nTrain,3))\n",
    "\n",
    "i = 0\n",
    "for inputs_batch, labels_batch in train_generator:\n",
    "    features_batch = vgg_conv.predict(inputs_batch)\n",
    "    train_features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "    train_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "    i += 1\n",
    "    if i * batch_size >= nTrain:\n",
    "        break\n",
    "         \n",
    "train_features = np.reshape(train_features, (nTrain, 8 * 8 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for validation data\n",
    "\n",
    "val_features = np.zeros(shape=(nVal, 8, 8, 512))\n",
    "val_labels = np.zeros(shape=(nVal,3))\n",
    "\n",
    "i = 0\n",
    "for inputs_batch, labels_batch in val_generator:\n",
    "    features_batch = vgg_conv.predict(inputs_batch)\n",
    "    val_features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "    val_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "    i += 1\n",
    "    if i * batch_size >= nVal:\n",
    "        break\n",
    "         \n",
    "val_features = np.reshape(val_features, (nVal, 8 * 8 * 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating classification layers\n",
    "\n",
    "Create a simple feedforward network with a:\n",
    "\n",
    "###### a. Relu activation function\n",
    "###### b. Dropout layer to deactivate half the neurons (minimize overfitting)\n",
    "###### c. Softmax output layer having 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=8 * 8 * 512))\n",
    "model.add(layers.Dropout(0.5)) # minimize overfitting (deactivates half of the neurons)\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compiling model with optimizer\n",
    "\n",
    "Configure the model for training, define optimizer to tune the hyperparameters, and defines performance metrics to be applied both in train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training model\n",
    "\n",
    "Train the model compiled with the train data and defines epochs, and calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 723 samples, validate on 239 samples\n",
      "Epoch 1/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.9212 - acc: 0.7621 - val_loss: 0.1503 - val_acc: 0.9498\n",
      "Epoch 2/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.2458 - acc: 0.9142 - val_loss: 0.0996 - val_acc: 0.9665\n",
      "Epoch 3/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.1225 - acc: 0.9613 - val_loss: 0.0940 - val_acc: 0.9707\n",
      "Epoch 4/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0814 - acc: 0.9723 - val_loss: 0.1900 - val_acc: 0.9372\n",
      "Epoch 5/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0684 - acc: 0.9737 - val_loss: 0.1164 - val_acc: 0.9665\n",
      "Epoch 6/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0678 - acc: 0.9723 - val_loss: 0.0594 - val_acc: 0.9791\n",
      "Epoch 7/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0228 - acc: 0.9903 - val_loss: 0.1195 - val_acc: 0.9665\n",
      "Epoch 8/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0227 - acc: 0.9945 - val_loss: 0.0670 - val_acc: 0.9874\n",
      "Epoch 9/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0193 - acc: 0.9945 - val_loss: 0.1837 - val_acc: 0.9372\n",
      "Epoch 10/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0236 - acc: 0.9931 - val_loss: 0.0814 - val_acc: 0.9791\n",
      "Epoch 11/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0126 - acc: 0.9972 - val_loss: 0.1365 - val_acc: 0.9582\n",
      "Epoch 12/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0198 - acc: 0.9903 - val_loss: 0.0806 - val_acc: 0.9833\n",
      "Epoch 13/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.1573 - val_acc: 0.9582\n",
      "Epoch 14/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0943 - val_acc: 0.9833\n",
      "Epoch 15/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0761 - val_acc: 0.9707\n",
      "Epoch 16/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0340 - acc: 0.9945 - val_loss: 0.1132 - val_acc: 0.9874\n",
      "Epoch 17/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0032 - acc: 0.9986 - val_loss: 0.1206 - val_acc: 0.9874\n",
      "Epoch 18/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0028 - acc: 0.9986 - val_loss: 0.1017 - val_acc: 0.9833\n",
      "Epoch 19/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.1036 - val_acc: 0.9874\n",
      "Epoch 20/20\n",
      "723/723 [==============================] - 2s 3ms/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.1105 - val_acc: 0.9874\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_features,\n",
    "                    train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(val_features,val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of errors = 157/239\n"
     ]
    }
   ],
   "source": [
    "fnames = val_generator.filenames\n",
    "ground_truth = val_generator.classes\n",
    "label2index = val_generator.class_indices\n",
    " \n",
    "# Getting the mapping from class index to class label\n",
    "idx2label = dict((v,k) for k,v in label2index.items())\n",
    " \n",
    "predictions = model.predict_classes(val_features)\n",
    "prob = model.predict(val_features)\n",
    " \n",
    "errors = np.where(predictions != ground_truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),nVal))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
