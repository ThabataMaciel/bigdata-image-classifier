{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing Images to generate a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-storage tensorflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from datetime import datetime as dt\n",
    "from multiprocessing import Pool\n",
    "from skimage.io import imread\n",
    "from tensorflow.keras.applications import vgg19\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/jovyan/work/bigdata-217213-55b1dfc31b66.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = dt.now().strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://bigdata-allanbatista-com-br/image-classifier/20181201_223811/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basepath = 'gs://bigdata-allanbatista-com-br/image-classifier/{}/'.format(train_id)\n",
    "basepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = storage.Client()\n",
    "bucket = gs.bucket('bigdata-allanbatista-com-br')\n",
    "\n",
    "def list_blobs(pattern):\n",
    "    return [blob.name for blob in bucket.list_blobs(prefix=pattern)]\n",
    "\n",
    "def list_images_with_labels(pattern):\n",
    "    x = []\n",
    "    y = []\n",
    "    for path in list_blobs(pattern):\n",
    "        x.append(path)\n",
    "        y.append(path.split(\"/\")[2])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, train_labels = list_images_with_labels(\"dataset/train\")\n",
    "test_paths, test_labels = list_images_with_labels(\"dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = LabelBinarizer()\n",
    "y_train = binarizer.fit_transform(train_labels)\n",
    "y_test = binarizer.transform(test_labels)\n",
    "\n",
    "with file_io.FileIO('{}binarizer.pickle'.format(basepath), 'wb+') as f:\n",
    "    f.write(pickle.dumps(binarizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vgg_conv = vgg19.VGG19(weights='imagenet',\n",
    "                       include_top=False,\n",
    "                       input_shape=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with file_io.FileIO('{}metadata.json'.format(basepath), 'wb+') as f:\n",
    "    f.write(json.dumps({\n",
    "        'input_dimention': 8 * 8 * 512,\n",
    "        'train_samples_count': len(y_train),\n",
    "        'test_samples_count': len(y_test),\n",
    "        'classes_count': len(binarizer.classes_)\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunck_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_features = np.array([])\n",
    "\n",
    "def create_record(features, label):\n",
    "    features = tf.train.Features(feature={\n",
    "        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=label)),\n",
    "        'features': tf.train.Feature(bytes_list=tf.train.BytesList(value=[features.tobytes()]))\n",
    "    })\n",
    "    \n",
    "    return tf.train.Example(features=features)\n",
    "\n",
    "def read_image(path):\n",
    "    filename = 'gs://bigdata-allanbatista-com-br/{}'.format(path)\n",
    "    with file_io.FileIO(filename, 'rb') as file:\n",
    "        image = imread(file)\n",
    "    \n",
    "    return image\n",
    "    \n",
    "def create_and_write_record(data, x, y, dest_path):\n",
    "    i = data[0]\n",
    "    index = data[1]\n",
    "    start = dt.now()\n",
    "    filename = \"{}{}.tfrecord\".format(dest_path, str(index).zfill(5))\n",
    "    \n",
    "    with Pool(10) as p:\n",
    "        images = np.array(p.map(read_image, x[i:i+chunck_size]))\n",
    "    \n",
    "    features = vgg_conv.predict(images)\n",
    "    features = np.reshape(features, (len(features), 8 * 8 * 512))\n",
    "    \n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for feature, label in zip(features, y[i:i+chunck_size]):\n",
    "            record = create_record(feature, label)\n",
    "            writer.write(record.SerializeToString())            \n",
    "\n",
    "    print(\"diff %ds: %s\" % ((dt.now() - start).total_seconds(), filename))\n",
    "\n",
    "    \n",
    "def create_dataset_to_gs(x, y, dest_path):\n",
    "    chunck_i = list(range(0, len(y), chunck_size))\n",
    "    chunck_index = list(range(len(chunck_i)))\n",
    "\n",
    "    for data in list(zip(chunck_i, chunck_index)):\n",
    "        create_and_write_record(data, x, y, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff 94s: gs://bigdata-allanbatista-com-br/image-classifier/20181201_223811/trainset/00000.tfrecord\n",
      "diff 91s: gs://bigdata-allanbatista-com-br/image-classifier/20181201_223811/trainset/00001.tfrecord\n"
     ]
    }
   ],
   "source": [
    "create_dataset_to_gs(train_paths, y_train, \"{}trainset/\".format(basepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_to_gs(test_paths, y_test, \"{}testset/\".format(basepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://bigdata-allanbatista-com-br/image-classifier/20181201_223811/'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basepath"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
